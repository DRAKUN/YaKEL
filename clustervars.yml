---

  #### -- vagrant vbox variables
vagrant:
  domain_tld: 'roklab.ops'
  box: "centos/7" 
  vbox_guest_version: '5.2.8'
  timezone: "Europe/Paris"
  ntp_area: "fr"
  gui:
    enabled: false
  ssh:
    forward_agent: true
    ssh_key_path: "~/.vagrant.d/insecure_private_key"
    insert_key: false
    shell: "bash -c 'BASH_ENV=/etc/profile exec bash'"
  vm_opts:
    ioapic: "on"
    dnsresolver: "on"
  hostmanager:
    enabled: true
    manage_host: true
    manage_guest: false
    offline_enabled: false
    private_ip_disabled: false
  landrush:
    enabled: true
    tld: roklab.ops
    interface: 'eth1'
    class: 'ipv4'
    redirect_guest: false
  sync:
    src: '.'
    dest: '/vagrant'
    id: 'vagrant-root'
    disabled: true

### -- kubernetes / ansible extra vars 

cluster:
    name: "local"
    domain: "cluster.local"
    kubernetes_version: 'v1.11.1'
    networking:
          pod_cidr_address: "172.16.0.0/16"
          service_cidr_address: "172.20.0.0/16"
          dns_service_ip: "172.20.0.2"

### -- kubernetes / ansible extra vars  
ingress:
    edge:
      route: apps.roklab.ops
      address: 192.168.32.8
      vagrant_enabled: false
      nodename: node02

apps:
    dashboard:
      enabled: false
    traefik:
      enabled: true
    spinnaker:
      enabled: false

### -- kubernetes / ansible extra vars / vagrant vmbox variables
server:
    etcd: ## etcd is for ansible only. etcd is on same vm as master
        nodes:
        - nodename: node01
          fqdn: 'node01.roklab.ops'
          vagrant_enabled: false # no need to create the vm, i just need it for host group logic in playbook
          ip: 192.168.32.7
    controlplane:
        nodes:
        - nodename: node01
          fqdn: node1.roklab.ops
          aliases: node01
          vagrant_enabled: true
          ip: 192.168.32.7
          cpu: 2
          ram: 2048
    worker:
        nodes:
        - nodename: node02
          fqdn: 'node02.roklab.ops'
          aliases: node02
          vagrant_enabled: true
          ip: 192.168.32.8
          cpu: 1
          ram: 1024
        - nodename: node03
          fqdn: node03.roklab.ops
          aliases: node03
          vagrant_enabled: true
          ip: 192.168.32.9
          cpu: 1
          ram: 1024

### -- vagrant ansible provisioner variables
provisioner:
    type: 'ansible'
    limit: "ROK8LABZ:localhost"
    extra_vars: 'clustervars.yml'
    verbose: "vv"
    config_file: 'provisioning/ansible.cfg'
    play:
      clusterplan: 'provisioning/clusterplan.yml'
      clusterkube: 'provisioning/clusterkube.yml'
      clusterapps: 'provisioning/clusterapps.yml'
    tags:
      plan: ["nmcli", "ipvs", "ntp", "hosts", "repo", "sysctl", "yum"]
      kube: ["controlplane", "certs", "certs_upload", "worker", "network", "dns", "kubeconfig"]
      apps: ["heapster", "dashboard", "traefik"]

### -- ansible groupvars & hostvars that get in the vagrant generated inventory --> (relative to Vagrantfile dir ) .vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory
    ansible_groups: {
        ROK8LABZ: ["node01", "node02", "node03"],
        'ROK8LABZ:children': ["cluster", "etcd", "storage", "ingress"],
        'ROK8LABZ:vars': { ansible_become: true, ansible_user: 'vagrant' },
        'cluster:children': ["controlplane", "worker"],
        'cluster:vars': { docker_version:  '18.03.0', authorization_modes: ['RBAC', 'Node'],
                          local_data_cache: "{{ playbook_dir }}/.ansible/assets",
                          remote_data_cache: "/usr/local/src/ansible/data"
                         },
        controlplane: ["node01"],
        'controlplane:vars': { secure_port: '6443', insecure_port: '8080' },
        worker: ["node02", "node03"],
        etcd: ["node01"],
        storage: ["node01"],
        ingress: ["node02"]
    }
    ansible_host_vars: {
                node01: { prefered_iface: '192.168.32.7', prefered_device: 'eth1'},
                node02: { prefered_iface: '192.168.32.8', prefered_device: 'eth1'},        
                node03: { prefered_iface: '192.168.32.9', prefered_device: 'eth1'}
    }
